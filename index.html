<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <meta name="theme-color" content="#000000" />
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="Open-Sans.css">
  <link rel="stylesheet" href="index.css">
  <title></title>
  <script defer="defer" src="./static/js/main.cb41f6a5.js"></script>
  <link href="./static/css/main.4017e162.css" rel="stylesheet">
  <meta name="description"
        content="Title: Title title title">
  <title>Title: Title title</title>
</head>

<body>
  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
        <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
        </a>
    </div>
    <div class="navbar-menu">
        <div class="navbar-start" style="flex-grow: 1; justify-content: center;">

            <div class="navbar-item has-dropdown is-hoverable">
                <a class="navbar-link">
                    More Research
                </a>
                <div class="navbar-dropdown">
                    <a class="navbar-item" href="https://loopyavatar.github.io/">
                        Loopy
                    </a>
                    <a class="navbar-item" href="https://cyberhost.github.io/">
                        CyberHost
                    </a>
                    <a class="navbar-item" href="https://omnihuman-lab.github.io/">
                      OmniHuman
                    </a>
                </div>
            </div>
        </div>

    </div>
  </nav>

  <div id="root" class="column-flex">
    <div id="title-flex" class="column-flex">
      <h1> AlignHuman: Improving Motion and Fidelity via Timestep-Segment Preference Optimization for Audio-Driven Human Animation </h1>
      <span>
        <a target="_blank" href="" onclick="return false;">Chao&nbsp;Liang</a><sup>1*</sup>,
        <a target="_blank" href="" onclick="return false;">Jianwen&nbsp;Jiang</a><sup>1*†</sup>,
        <a target="_blank" href="" onclick="return false;">Wang&nbsp;Liao</a><sup>1*</sup>,
        <a target="_blank" href="" onclick="return false;">Jiaqi&nbsp;Yang</a><sup>1</sup>,
        <a target="_blank" href="" onclick="return false;">Zerong&nbsp;Zheng</a><sup>1</sup>,
        <a target="_blank" href="" onclick="return false;">Weihong&nbsp;Zeng</a><sup>1</sup>,
        <a target="_blank" href="" onclick="return false;">Han&nbsp;Liang</a><sup>1</sup>,
        <br />
      </span>
      <span><sup>1</sup>Bytedance</span>
      <span><sup>*</sup>Equal contribution,<sup>†</sup>Project lead</span>
      <div class="flex flex-gap" style="margin-bottom:0.5em;">
        <a target="_blank" href="https://alignhuman.github.io" ><button>Paper</button></a>
	      <!-- <a target="_blank" href="" onclick="alert('To be determined');return false;"><button>Code</button></a> -->
        <a target="_blank" href="https://alignhuman.github.io"><button>Page</button></a>
      </div>
      <small><span><b>TL;DR</b>: Recent advancements in human video generation and animation tasks, driven by diffusion models, have achieved significant progress. However, expressive and realistic human animation remains challenging due to the trade-off between motion naturalness and visual fidelity. To address this, we propose <b>AlignHuman</b>, a framework that combines Preference Optimization as a post-training technique with a divide-and-conquer training strategy to jointly optimize these competing objectives.  
        Our key insight stems from an analysis of the denoising process across timesteps: (1) early denoising timesteps primarily control motion dynamics, while (2) fidelity and human structure can be effectively managed by later timesteps, even if early steps are skipped. Building on this observation, we propose timestep-segment preference optimization (TPO) and introduce two specialized LoRAs as expert alignment modules, each targeting a specific dimension in its corresponding timestep interval. The LoRAs are trained using their respective preference data and activated in the corresponding intervals during inference to enhance motion naturalness and fidelity.
        Extensive experiments demonstrate that AlignHuman improves strong baselines and reduces NFEs during inference, achieving a 3.3&times; speedup (from 100 NFEs to 30 NFEs) with minimal impact on generation quality.</span></small>
      <div class='responsive-image-container'>
        <img src='image/teaser.png' alt='' />
      </div>
    </div>

    <div id="sections" class="column-flex">
      <!-- <p style="color:#700000"><i>(Note: all portrait images on this page are virtual, non-existing identities generated by StyleGAN2 or DALL·E-3 (except for Mona Lisa). We are exploring visual affective skill generation for virtual, interactive characters, NOT impersonating any person in the real world. This is only a research demonstration and there's no product or API release plan. See also the bottom of this page for more of our Responsible AI considerations.) </i></p> -->
      <h3>Generated Videos at 30 NFEs</h3>
        <p>
          AlignHuman is an audio-driven human animation framework, supporting various visual (cartoons, portraits, whole-body, any aspect-ratios, .etc) and audio (sing, talk) styles. It addresses the challenge of balancing motion naturalness and visual fidelity. 
        </p>
        <div class="video-slider">
          <video src="video/main_14.mp4"></video>
          <video src="video/main_2.mp4"></video>
          <video src="video/main_1.mp4"></video>
          <!-- <video src="video/main_5.mp4"></video> -->
        </div>
        <div class="video-slider">
          <video src="video/main_10.mp4"></video>
          <video src="video/main_11.mp4"></video>
          <video src="video/main_12.mp4"></video>
          <!-- <video src="video/main_5.mp4"></video> -->
        </div>
        <div class="video-slider">
          <video src="video/main_3.mp4"></video>
          <video src="video/main_4.mp4"></video>
          <video src="video/main_7.mp4"></video>
          <!-- <video src="video/main_5.mp4"></video> -->
        </div>
        <div class="video-slider">
          <video src="video/main_15.mp4"></video>
          <video src="video/main_16.mp4"></video>
          <video src="video/main_17.mp4"></video>
          <!-- <video src="video/main_5.mp4"></video> -->
        </div>

      <h3>30 NFEs vs 100 NFEs</h3>
        <p>
          <!-- Loopy can generate motion-adapted synthesis results for the same reference image based on different audio inputs, whether they are rapid, soothing, or realistic singing performances. -->
        </p>
        <div class="video-container">
          <video controls playsInline src="video/b_30NFEs_vs_100NFEs_1.mov"></video>
        </div>
        <div class="video-container">
          <video controls playsInline src="video/b_30NFEs_vs_100NFEs_2.mov"></video>
        </div>
        <div class="video-container">
          <video controls playsInline src="video/b_30NFEs_vs_100NFEs_3.mov"></video>
        </div>
        <!-- <div class="video-container">
          <video controls playsInline src="video/b_30NFEs_vs_100NFEs_4.mov"></video>
        </div> -->
        <!-- <div class="video-slider"> -->
          
        



      <h3>Comparison with Other Methods</h3>
        <div class="video-container">
          <video controls playsInline src="video/a_vs_baselines_1.mov"></video>
        </div>
        <div class="video-container">
          <video controls playsInline src="video/a_vs_baselines_2.mp4"></video>
        </div>
        <div class="video-container">
          <video controls playsInline src="video/a_vs_baselines_3.mp4"></video>
        </div>

      <h3>Ethics Concerns</h3>
        <p>
          The purpose of this work is only for research. The images and audios used in these demos are from AIGC tools. If there are any concerns, please contact us and we will delete it in time.
        </p>

      
      
      <!-- <h3>BibTeX</h3>
        <p>If you find this project is useful to your research, please cite us:</p>
        <pre><code>

        </code></pre> -->

      <br/>
      <br/>
      <br/>
    </div>
  </div>
  <script src="index.js"></script>
  <script>
    function comming_soon_click() {
      alert('Comming soon!');
    }
    function TBD_click() {
      alert('TBD');
    }
  </script>
</body>



</html>
